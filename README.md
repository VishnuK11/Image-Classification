# Image Classification
 
This Repository is a collection of all image classification projects. 

## [1. Simpsons Character Recognition:](https://github.com/VishnuK11/Image-Classification/tree/main/Simpsons%20Character%20Recognition)
The Simpsons Character Dataset is a collection of 21K images of 42 characters of the Simpsons show. The images concerning the top 20 characters of the dataset are chosen for this project, amounting to 19K images. The model architecture containts 7 blocks of layers. Each block has a one padding layer, a batch normalization layer and about 'K' Conv2d layers for the Kth block. The model performed excellent, with 95% training accuracy, 92% validation accuracy and 95% test accuracy.

## [2. Fashion MNIST:](https://github.com/VishnuK11/Image-Classification/tree/main/Fashion%20MNIST)
The Fashion MNIST image classification is a classical image classification excercise. It consists of 10 classes of fashion items in dataset of 60K training images and 10K test images. In this notebook, I've explored a step-by-step approach on the various model architectures starting with a vanilla neural network. The test dataset was divided into validation and test subsets consisting of 2K and 8K images respectively. By looking at the Accuracy - Epoch and Loss-Epoch plots, I have gradually made necessary changes in the  model architecture to get a 2%-3% improvement on the test performance. The notebook contains 6 architectures. A simple 5 layer Neural Network, a 5 layer CNN, CNN+dropout, CNN+L2Reg, CNN+dropout+L2Reg, CNN+Augmentation+dropout+L2Reg. A comparative analysis and observations are detailed in the notebook. The final model performed with an accuracy of 92% compared to the inital performance of 89% on the test dataset.

## [3. MNIST Hand Written Digits:](https://github.com/VishnuK11/Image-Classification/tree/main/Digits%20MNIST)
The MNIST Digits Dataset is a collection of 60K train images of 10 handwritten digits 0-9. In this notebook, I compare the performance of 2 models one based on classical Neural Network and another on CNN architecture. For both models, Adam optimizer and Sparse Categoriccal Crossentropy loss function were used. The models were trained for 10-20 epochs. Dropout and L2 regularization were used on the CNN model. Calls Backs with early stopping and LR reduce on Plateau were implemented. The model performed excellent on the test data with an accuracy of 98% for the simple NN and 99.5% for the CNN architecture. Based on error analysis, most errors were due to mislabelling of classes. A few examples had images that were harder to classify even for the human eye, like a 1-7 or a 6-8.  These can be be considered corner cases. Hence, this accuracy can be considered on par to a Human.

## [4. HyperParameter Tuning using RL:](https://github.com/VishnuK11/Image-Classification/tree/main/HyperParam%20Tuning%20Using%20RL%20MNIST)
Hyper parameter selection and tuning is a challenging task. I wanted to explore the feasibility of applying Reinforcement learning based on Q Learning to determine the hyperparameters like Droupout, Learning Rate and L2 Regularizations. I used a predetermined architecture for classifying the MNIST Digits Dataset. The reward was validation accuracy and to limit compute resources, the epoch was fixed to 2. Using this method, I was able to select the right Hyperparameter for this task. For this data and the architecture selected, the best hyper parameters are Dropout = 0.10,  L2Reg = 0.001, and LR = 0.01.
